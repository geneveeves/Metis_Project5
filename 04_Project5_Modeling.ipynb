{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickled df with all data\n",
    "df = pd.read_pickle('./all_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 345050 entries, 0 to 359341\n",
      "Data columns (total 10 columns):\n",
      "country_name                345050 non-null object\n",
      "region_name                 345050 non-null object\n",
      "income_group_name           345050 non-null object\n",
      "implementing_agency_name    345050 non-null object\n",
      "assistance_category_name    345050 non-null object\n",
      "transaction_type_name       345050 non-null object\n",
      "fiscal_year                 345050 non-null int64\n",
      "constant_amount             345050 non-null int64\n",
      "USG_sector_name             345050 non-null object\n",
      "HDI_Change                  345050 non-null float64\n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 29.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# I think I need to create like 'totals' --> aggregate sums for same country/year/purpose\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your feature matrix (X) and target vector (y)\n",
    "#y, X = patsy.dmatrices('Y ~ X1 + X2 + X3 + X4 + X5 + X6', data=df, return_type=\"dataframe\")\n",
    "\n",
    "# Create your model\n",
    "#model = sm.OLS(y, X)\n",
    "\n",
    "# Fit your model to your training set\n",
    "#fit = model.fit()\n",
    "\n",
    "# Print summary statistics of the model's performance\n",
    "#fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation/collinearity\n",
    "# example of a better corr matrix\n",
    "#sns.heatmap(df.corr(), cmap=\"seismic\", annot=True, vmin=-1, vmax=1);\n",
    "# more cmaps: https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "# Plot all of the variable-to-variable relations as scatterplots\n",
    "#sns.pairplot(df, height=1.2, aspect=1.5);?\n",
    "#Split the data 60 - 20 - 20 train/val/test\n",
    "\n",
    "#X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=.25, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso_model = Lasso(alpha = 1000000) # this is a VERY HIGH regularization strength!, wouldn't usually be used\n",
    "#lasso_model.fit(X_train.loc[:,selected_columns], y_train)\n",
    "#list(zip(selected_columns, lasso_model.coef_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######LassoCV\n",
    "# Run the cross validation, find the best alpha, refit the model on all the data with that alpha\n",
    "\n",
    "#alphavec = 10**np.linspace(-2,2,200)\n",
    "\n",
    "#lasso_model = LassoCV(alphas = alphavec, cv=5)\n",
    "#lasso_model.fit(X_tr, y_train)\n",
    "\n",
    "# This is the best alpha value it found - not far from the value\n",
    "# selected using simple validation\n",
    "#lasso_model.alpha_\n",
    "\n",
    "\n",
    "# These are the (standardized) coefficients found\n",
    "# when it refit using that best alpha\n",
    "#list(zip(X_train.columns, lasso_model.coef_))\n",
    "\n",
    "# Make predictions on the test set using the new model\n",
    "#test_set_pred = lasso_model.predict(X_te)\n",
    "\n",
    "#r2_score(y_test, test_set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_model_ridge = Ridge(alpha = 1000000000000)\n",
    "#lr_model_ridge.fit(X_train_collinear, y_train)\n",
    "\n",
    "#list(zip(X_train_collinear.columns, lr_model_ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**LASSO**:\n",
    "#* _Pro_: great for trimming features and focusing interpretation on a few key ones\n",
    "#* _Con_: risk of discarding features that are actually useful\n",
    "\n",
    "#**Ridge**:\n",
    "#* _Pro_: great for smoothly handling multicollinearity, very nice when working with sparse features \n",
    "#* _Con_: will never fully discard features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standard Scaling\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## This step fits the Standard Scaler to the training data\n",
    "## Essentially it finds the mean and standard deviation of each variable in the training set\n",
    "\n",
    "#std = StandardScaler()\n",
    "#std.fit(X_train.values)\n",
    "\n",
    "## This step applies the scaler to the train set.\n",
    "## It subtracts the mean it learned in the previous step and then divides by the standard deviation\n",
    "\n",
    "#X_tr = std.transform(X_train.values)\n",
    "\n",
    "## Apply the scaler to the test set\n",
    "\n",
    "#X_te = std.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to try:\n",
    "##### OLS\n",
    "##### Linear Reg\n",
    "##### Polynomial Reg?\n",
    "##### ElasticNetCV: useful when there are multiple features which are correlated\n",
    "##### Ridge\n",
    "##### Lasso\n",
    "\n",
    "##### RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Forest Regressor\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#def rfr_model(X, y):\n",
    "# Perform Grid-Search\n",
    "#    gsc = GridSearchCV(\n",
    "#        estimator=RandomForestRegressor(),\n",
    "#        param_grid={\n",
    "#            'max_depth': range(3,7),\n",
    "#            'n_estimators': (10, 50, 100, 1000),\n",
    "#        },\n",
    "#        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "#    grid_result = gsc.fit(X, y)\n",
    "#    best_params = grid_result.best_params_\n",
    "    \n",
    "#    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], \n",
    "#          n_estimators=best_params[\"n_estimators\"], random_state=False, verbose=False)\n",
    "\n",
    "# Perform K-Fold CV\n",
    "#   scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "#    return scores\n",
    "\n",
    "#Here, we have chosen the two hyperparameters; max_depth and n_estimators, \n",
    "#to be optimized. According to sklearn documentation, max_depth refers to the maximum \n",
    "#depth of the tree and n_estimators, the number of trees in the forest. \n",
    "\n",
    "\n",
    "\n",
    "# After creating a random forest regressor object, we pass it to the cross_val_score() \n",
    "# function which performs K-Fold cross validation on the given data and provides as an \n",
    "#output, an error metric value, which can be used to determine the model performance.\n",
    "\n",
    "#scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# The lower the MAE is, the better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
